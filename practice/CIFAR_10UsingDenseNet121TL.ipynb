{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSHYzGFQ_8Wh"
   },
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tVn0COO__lWC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O6mJc4YqAV8L"
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FO0R6IbVAq2i"
   },
   "outputs": [],
   "source": [
    "# DenseNet was trained on ImageNet, so we use its mean/std for normalization.\n",
    "# We also resize the CIFAR-10's 32x32 images to the 224x224 expected by the pre-trained model.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224), # Resize images to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ze6aX1bXAtQK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHX50VS7A9qR"
   },
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cuLvjkvxA5Ww"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 170M/170M [00:03<00:00, 43.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 data\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducing the number of images for faster training\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "SUBSET_SIZE = 5000  # Using only 5,000 images instead of 50,000\n",
    "\n",
    "# Get the indices for the subset\n",
    "indices = random.sample(range(len(train_dataset)), SUBSET_SIZE)\n",
    "\n",
    "# Create the new reduced training dataset\n",
    "train_subset = Subset(train_dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SUBSET_SIZE = 1000  # Example: Use 1,000 images instead of 10,000\n",
    "\n",
    "# Get the indices for the subset\n",
    "indices = random.sample(range(len(test_dataset)), TEST_SUBSET_SIZE)\n",
    "\n",
    "# Create the new reduced test dataset\n",
    "test_subset = Subset(test_dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CfCw57ccBDB5"
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "i6JhXtNxBGja"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 79\n",
      "Number of testing batches: 16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUUOcjRcBR_o"
   },
   "source": [
    "Build the DenseNet121 Model (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9IjoKeNoBHAl"
   },
   "outputs": [],
   "source": [
    "def setup_densenet_model(num_classes):\n",
    "  # Load the pre-trained DenseNet121 model\n",
    "  model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "\n",
    "  # Freeze all the parameters (feature extractor layers)\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  # Replace the final classification layer (classifier)\n",
    "  # The original classifier has a linear layer whose input is num_features\n",
    "  num_ftrs = model.classifier.in_features\n",
    "\n",
    "  # Create a new classifier for 10 classes\n",
    "  model.classifier = nn.Sequential(\n",
    "      nn.Linear(num_ftrs, 512), # Optional intermediate layer\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.5),\n",
    "      nn.Linear(512, num_classes) # Final layer for 10 classes\n",
    "  )\n",
    "\n",
    "  # Move model to the selected device\n",
    "  model = model.to(device)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "96qi2QeWHxsg"
   },
   "outputs": [],
   "source": [
    "# Setup the model\n",
    "model = setup_densenet_model(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kzYjflfTH5_q"
   },
   "outputs": [],
   "source": [
    "# Define Loss function and Optimizer (only for the unfrozen layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# We pass model.parameters() to the optimizer, but only the parameters\n",
    "# of the new classifier head have requires_grad=True.\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pWt0g6zIUV4"
   },
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "y9_zvWwbIQQA"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, epochs):\n",
    "  history = {'train_loss': [], 'train_acc': []}\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train() # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "      # Zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      # Backward pass and optimize\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Statistics\n",
    "      running_loss += loss.item() * inputs.size(0)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      total_samples += labels.size(0)\n",
    "      correct_preds += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = correct_preds/total_samples\n",
    "\n",
    "    history['train_loss'].append(epoch_loss)\n",
    "    history['train_acc'].append(epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc*100:.2f}%\")\n",
    "\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "963H8tLmKdOI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Loss: 1.3436 | Accuracy: 52.98%\n"
     ]
    }
   ],
   "source": [
    "#running the training\n",
    "history = train_model(model, criterion, optimizer, train_loader, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIXy-QvdKscg"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "p0u0rpU5KnRk"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "  model.eval() # Set the model to evaluation model\n",
    "  correct_preds = 0\n",
    "  total_samples = 0\n",
    "\n",
    "  with torch.no_grad(): # Disable gradient calculations during testing\n",
    "    for inputs, labels in test_loader:\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      total_samples += labels.size(0)\n",
    "      correct_preds += (predicted == labels).sum().item()\n",
    "\n",
    "  accuracy = correct_preds / total_samples\n",
    "  print(f\"\\nFinal Test Accuracy: {accuracy*100:.2f}%\")\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Q2kRQzKUL9Qr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Accuracy: 73.50%\n"
     ]
    }
   ],
   "source": [
    "#run the evaluation\n",
    "test_accuracy = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JmejfMwZMJwt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.735\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
